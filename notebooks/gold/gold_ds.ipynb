{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema_landing = dbutils.widgets.get(\"schema_landing\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "schema_gold = dbutils.widgets.get(\"schema_gold\")\n",
    "volume = dbutils.widgets.get(\"volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85bc3d",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a385654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Load silver Delta table \n",
    "# ----------------------------\n",
    "\n",
    "source_table = f\"{catalog}.{schema_silver}.weather_clean\"\n",
    "silver_df = spark.readStream.table(source_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc10a20",
   "metadata": {},
   "source": [
    "**DS GOLD TABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05c51c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table_ds = f\"{catalog}.{schema_gold}.ds_weather_hourly\"\n",
    "checkpoint_ds = f\"/Volumes/{catalog}/{schema_landing}/{volume}/checkpoints/iot_gold/df_ds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ca261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds = (\n",
    "    silver_df\n",
    "    .groupBy(\n",
    "        F.col(\"city_id\"),\n",
    "        F.col(\"name\").alias(\"city_name\"),\n",
    "        F.col(\"country\").alias(\"country_code\"),\n",
    "        F.window(F.col(\"local_time\"), \"1 hour\").alias(\"time_window\")\n",
    "    )\n",
    "    .agg(\n",
    "        # --- Temperature Stats ---\n",
    "        F.round(F.avg(\"temperature\"), 2).alias(\"temp_mean\"),\n",
    "        F.max(\"temperature\").alias(\"temp_max\"),\n",
    "        F.min(\"temperature\").alias(\"temp_min\"),\n",
    "        F.round(F.stddev(\"temperature\"), 3).alias(\"temp_std\"),\n",
    "\n",
    "        # --- Humidity & Pressure ---\n",
    "        F.round(F.avg(\"humidity\"), 1).alias(\"humidity_mean\"),\n",
    "        F.round(F.stddev(\"humidity\"), 2).alias(\"humidity_std\"),\n",
    "        F.round(F.avg(\"pressure\"), 1).alias(\"pressure_mean\"),\n",
    "\n",
    "        # --- Wind ---\n",
    "        F.round(F.avg(\"windspeed\"), 2).alias(\"wind_speed_mean\"),\n",
    "\n",
    "        # --- Totals ---\n",
    "        F.sum(F.coalesce(\"rain_1h\", F.lit(0))).alias(\"rain_total_mm\"),\n",
    "        F.round(F.avg(\"clouds_all\"), 1).alias(\"clouds_avg_pct\")\n",
    "    )\n",
    "    # Extract window start/end for proper time alignment\n",
    "    .withColumn(\"hour_local_start\", F.col(\"time_window.start\"))\n",
    "    .withColumn(\"hour_local_end\", F.col(\"time_window.end\"))\n",
    "    .drop(\"time_window\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f44d5307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x16e3c4af890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#complete mode is required for aggregations, but we can optimize by only writing the final results to the target table, and using checkpointing to manage state\n",
    "(df_ds.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpoint_ds)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(target_table_ds)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28726cdc",
   "metadata": {},
   "source": [
    "**DS GOLD: Optimize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e510989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>{'numFilesAdded': 0, 'numFilesRemoved': 0, 'filesAdded': {'min': None, 'max': None, 'avg': 0.0, 'totalFiles': 0, 'totalSize': 0}, 'filesRemoved': {'min': None, 'max': None, 'avg': 0.0, 'totalFiles': 0, 'totalSize': 0}, 'partitionsOptimized': 0, 'zOrderStats': None, 'clusteringStats': None, 'numBins': 0, 'numBatches': 0, 'totalConsideredFiles': 0, 'totalFilesSkipped': 0, 'preserveInsertionOrder': True, 'numFilesSkippedToReduceWriteAmplification': 0, 'numBytesSkippedToReduceWriteAmplification': 0, 'startTimeMs': 1770579858558, 'endTimeMs': 1770579859064, 'totalClusterParallelism': 8, 'totalScheduledTasks': 0, 'autoCompactParallelismStats': None, 'deletionVectorStats': {'numDeletionVectorsRemoved': 0, 'numDeletionVectorRowsRemoved': 0}, 'recompressionCodec': None, 'numTableColumns': 15, 'numTableColumnsWithStats': 15, 'totalTaskExecutionTimeMs': 0, 'skippedArchivedFiles': 0, 'clusteringMetrics': None}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,recompressionCodec:string,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,isFull:boolean,approxClusteringQuality:double,approxClusteringQualityPerColumn:array<double>,approxClusteringCoverage:double,compactionType:string,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numIdealFilesWithTrimmedStringMaxValue:bigint,numAddedFilesWithSameMinMaxOnClusteringColumns:array<bigint>,numClusteringTasksPlanned:int,numClusteringTasksNotPlannedDueToPO:int,numCompactionTasksPlanned:int,numCompactionTasksPlannedUndoneDueToPO:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numLeafNodesCompactedUndoneDueToPO:bigint,numIntermediateNodesCompacted:bigint,numIntermediateNodesCompactedUndoneDueToPO:bigint,totalSizeOfDataToCompactInBytes:bigint,totalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"OPTIMIZE {catalog}.{schema_gold}.ds_weather_hourly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
