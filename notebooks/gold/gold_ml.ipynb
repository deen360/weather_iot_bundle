{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema_landing = dbutils.widgets.get(\"schema_landing\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "schema_gold = dbutils.widgets.get(\"schema_gold\")\n",
    "volume = dbutils.widgets.get(\"volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85bc3d",
   "metadata": {},
   "source": [
    "**LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a385654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Load silver Delta table \n",
    "# ----------------------------\n",
    "\n",
    "source_table = f\"{catalog}.{schema_silver}.weather_clean\"\n",
    "silver_df = spark.readStream.table(source_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28726cdc",
   "metadata": {},
   "source": [
    "**ML GOLD:for Predictive Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "108c0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table_ml = f\"{catalog}.{schema_gold}.ml_weather_features\"\n",
    "checkpoint_ml = f\"/Volumes/{catalog}/{schema_landing}/{volume}/checkpoints/iot_gold/df_ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beec5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = (\n",
    "    silver_df\n",
    "    .groupBy(\n",
    "        F.col(\"city_id\"),\n",
    "        F.col(\"name\").alias(\"city_name\"),\n",
    "        F.col(\"country\").alias(\"country_code\"),\n",
    "        F.window(\"local_time\", \"1 hour\").alias(\"hour_window\")\n",
    "    )\n",
    "    .agg(\n",
    "        # --- Core Numeric Stats ---\n",
    "        F.mean(\"temperature\").alias(\"temp_mean\"),\n",
    "        F.expr(\"percentile_approx(temperature, 0.5)\").alias(\"temp_median\"),\n",
    "        F.expr(\"mode(temperature)\").alias(\"temp_mode\"), \n",
    "\n",
    "        F.mean(\"humidity\").alias(\"humidity_mean\"),\n",
    "        F.expr(\"percentile_approx(humidity, 0.5)\").alias(\"humidity_median\"),\n",
    "        F.expr(\"mode(humidity)\").alias(\"humidity_mode\"), \n",
    "\n",
    "        F.mean(\"pressure\").alias(\"pressure_mean\"),\n",
    "        F.expr(\"percentile_approx(pressure, 0.5)\").alias(\"pressure_median\"),\n",
    "        F.expr(\"mode(pressure)\").alias(\"pressure_mode\"), # FIXED\n",
    "\n",
    "        F.mean(\"windspeed\").alias(\"windspeed_mean\"),\n",
    "        F.expr(\"percentile_approx(windspeed, 0.5)\").alias(\"windspeed_median\"),\n",
    "        F.expr(\"mode(windspeed)\").alias(\"windspeed_mode\"), # FIXED\n",
    "\n",
    "        # --- Rain (Handle Nulls) ---\n",
    "        F.mean(F.coalesce(\"rain_1h\", F.lit(0))).alias(\"rain_mean\"),\n",
    "        F.expr(\"percentile_approx(coalesce(rain_1h, 0), 0.5)\").alias(\"rain_median\"),\n",
    "        F.expr(\"mode(rain_1h)\").alias(\"rain_mode\") # FIXED\n",
    "    )\n",
    "\n",
    "    .select(\n",
    "        # 1. Extract Window Columns\n",
    "        F.col(\"hour_window.start\").alias(\"event_hour_local\"),\n",
    "        F.col(\"hour_window.end\").alias(\"event_hour_end\"),\n",
    "        \n",
    "        # 2. Select Everything Else (This includes city_id, city_name, country_code)\n",
    "        \"*\" \n",
    "    )\n",
    "    # 3. Drop the struct column (since we extracted start/end)\n",
    "    .drop(\"hour_window\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970b90fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x1a5b2dd4560>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_ml.writeStream\n",
    "    .format(\"delta\")\n",
    "    \n",
    "    # CRITICAL: 'complete' mode is required for Aggregations without Watermark.\n",
    "    # It means: \"Write the current calculated value for every window.\"\n",
    "    # Delta Lake handles the \"Upsert\" (updating the existing row) automatically.\n",
    "    .outputMode(\"complete\")\n",
    "    \n",
    "    .option(\"checkpointLocation\", checkpoint_ml)\n",
    "    \n",
    "    # TRIGGER: Process all available data as a batch, then stop.\n",
    "    .trigger(availableNow=True)\n",
    "    \n",
    "    .toTable(target_table_ml)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e510989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>{'numFilesAdded': 0, 'numFilesRemoved': 0, 'filesAdded': {'min': None, 'max': None, 'avg': 0.0, 'totalFiles': 0, 'totalSize': 0}, 'filesRemoved': {'min': None, 'max': None, 'avg': 0.0, 'totalFiles': 0, 'totalSize': 0}, 'partitionsOptimized': 0, 'zOrderStats': None, 'clusteringStats': None, 'numBins': 0, 'numBatches': 0, 'totalConsideredFiles': 0, 'totalFilesSkipped': 0, 'preserveInsertionOrder': True, 'numFilesSkippedToReduceWriteAmplification': 0, 'numBytesSkippedToReduceWriteAmplification': 0, 'startTimeMs': 1770579863454, 'endTimeMs': 1770579865657, 'totalClusterParallelism': 8, 'totalScheduledTasks': 0, 'autoCompactParallelismStats': None, 'deletionVectorStats': {'numDeletionVectorsRemoved': 0, 'numDeletionVectorRowsRemoved': 0}, 'recompressionCodec': None, 'numTableColumns': 20, 'numTableColumnsWithStats': 20, 'totalTaskExecutionTimeMs': 0, 'skippedArchivedFiles': 0, 'clusteringMetrics': None}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,recompressionCodec:string,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,isFull:boolean,approxClusteringQuality:double,approxClusteringQualityPerColumn:array<double>,approxClusteringCoverage:double,compactionType:string,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numIdealFilesWithTrimmedStringMaxValue:bigint,numAddedFilesWithSameMinMaxOnClusteringColumns:array<bigint>,numClusteringTasksPlanned:int,numClusteringTasksNotPlannedDueToPO:int,numCompactionTasksPlanned:int,numCompactionTasksPlannedUndoneDueToPO:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numLeafNodesCompactedUndoneDueToPO:bigint,numIntermediateNodesCompacted:bigint,numIntermediateNodesCompactedUndoneDueToPO:bigint,totalSizeOfDataToCompactInBytes:bigint,totalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"OPTIMIZE {catalog}.{schema_gold}.ml_weather_features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
