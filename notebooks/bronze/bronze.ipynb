{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession  \n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import current_timestamp, col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema_landing = dbutils.widgets.get(\"schema_landing\")\n",
    "schema_bronze = dbutils.widgets.get(\"schema_bronze\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "schema_gold = dbutils.widgets.get(\"schema_gold\")\n",
    "volume = dbutils.widgets.get(\"volume\")\n",
    "BUCKET_NAME = dbutils.widgets.get(\"bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ae373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cloud Path: s3a://my-databrick-iot-deen-001/raw_weather/\n"
     ]
    }
   ],
   "source": [
    "#Bucket path-\n",
    "cloud_path = f\"s3a://{BUCKET_NAME}/raw_weather/\"\n",
    "\n",
    "# UC Volume used for checkpoints (stream state) #volume/catalog/schemas\n",
    "checkpoint_path = f\"/Volumes/{catalog}/{schema_landing}/{volume}/checkpoints/iot_bronze/\"\n",
    "\n",
    "# UC Volume used for Auto Loader schema tracking (required for schema drift)\n",
    "schema_path = f\"/Volumes/{catalog}/{schema_landing}/{volume}/schemas/iot_bronze/\"\n",
    "\n",
    "# Bronze table (Unity Catalog managed Delta table)\n",
    "bronze_table = f\"{catalog}.{schema_bronze}.iot_bronze_weather\"\n",
    "\n",
    "#print(f\"✅ Cloud Path: {cloud_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INGESTION CONFIGURATION ---\n",
    "# We use Auto Loader (cloudFiles) for high-frequency IoT data \n",
    "# It handles schema drift and scales via file notification mode.\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "        # 1. Format & Inference: Ingest raw JSON. \n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "\n",
    "        # 2. Schema Management: Persistence is key for Unity Catalog.\n",
    "        # This allows the stream to restart and remember the previous schema.\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "\n",
    "        # 3. Precision Control: Force the IoT timestamp to high-precision TIMESTAMP \n",
    "        # to prevent it being misidentified as a STRING during inference.\n",
    "        .option(\"cloudFiles.schemaHints\", \"ingest_timestamp TIMESTAMP\")\n",
    "\n",
    "        # 4. Processing Strategy mode. \n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"true\")  #set to true to process existing files and false to ignore existing files\n",
    "\n",
    "        # 5. Source Path: Map to the Unity Catalog External Volume.\n",
    "        .load(cloud_path)\n",
    "\n",
    "        # --- AUDIT & LINEAGE METADATA ---\n",
    "        # Capture the processing time and source file path for debugging and data lineage.\n",
    "        .withColumn(\"_ingest_timestamp\", current_timestamp())\n",
    "        .withColumn(\"_ingest_file_name\", col('_metadata.file_name'))\n",
    "        .withColumn(\"_ingest_file_path\", col('_metadata.file_path'))\n",
    "\n",
    "        # --- PARTITIONING & OPTIMIZATION ---\n",
    "        # Generate a DATE column from the timestamp. \n",
    "        # This will be used for partitioning the Bronze table to optimize storage and cost.\n",
    "        .withColumn(\"ingestion_date\", to_date(\"_ingest_timestamp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2142750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x28f93c5f350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\") # Explicitly define that we are only adding new rows\n",
    "        \n",
    "        # 1. Reliability: Essential for tracking progress in External Volumes\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "\n",
    "        # 2. Evolution: Ensures the Delta Table structure updates when sensors change\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "\n",
    "        # 3. Layout: Physcially organizes data on disk by date for faster cleanup/queries\n",
    "        .partitionBy(\"ingestion_date\")\n",
    "\n",
    "        # 4. Cost Control: Processes all pending files and then stops the cluster\n",
    "        .trigger(availableNow=True)\n",
    "\n",
    "        # 5. Destination: The Unity Catalog Three-Level Namespace (catalog.schema.table)\n",
    "        .toTable(bronze_table) \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
