{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession  #ruung in db\n",
    "spark = DatabricksSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CERATe THE CATALOG\n",
    "spark.sql(\"\"\"\n",
    "CREATE CATALOG IF NOT EXISTS iot_catalog\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE Landing SCHEMA\n",
    "spark.sql(\"\"\"CREATE SCHEMA IF NOT EXISTS iot_catalog.00_landing\n",
    "COMMENT 'Landing zone for raw IoT sensor JSON files'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create volume to prevennt to store checkpoints\n",
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS iot_catalog.00_landing.source_iot_data\n",
    "COMMENT 'Landing zone for raw IoT sensor JSON files'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE Bronze SCHEMA\n",
    "spark.sql(\"\"\" CREATE SCHEMA IF NOT EXISTS iot_catalog.01_bronze\n",
    "COMMENT 'Bronze zone for raw IoT sensor JSON files' \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10efce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. GET PARAMETERS ---\n",
    "# We retrieve the bucket name \n",
    "BUCKET_NAME = \"my-databrick-iot-deen-001\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ae373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucket path-\n",
    "cloud_path = f\"s3a://{BUCKET_NAME}/raw_weather/\"\n",
    "\n",
    "# UC Volume used for checkpoints (stream state) #volume/catalog/schemas\n",
    "checkpoint_path = \"/Volumes/iot_catalog/00_landing/source_iot_data/checkpoints/iot_bronze/\"\n",
    "\n",
    "# UC Volume used for Auto Loader schema tracking (required for schema drift)\n",
    "schema_path = \"/Volumes/iot_catalog/00_landing/source_iot_data/schemas/iot_bronze/\"\n",
    "\n",
    "# Bronze table (Unity Catalog managed Delta table)\n",
    "bronze_table = \"iot_catalog.01_bronze.iot_bronze_weather\"\n",
    "\n",
    "print(f\"âœ… Cloud Path: {cloud_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INGESTION CONFIGURATION ---\n",
    "# We use Auto Loader (cloudFiles) for high-frequency IoT data because it \n",
    "# handles schema drift and scales via file notification mode.\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "        # 1. Format & Inference: Ingest raw JSON. \n",
    "        # Sampling is used initially to determine data types.\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "\n",
    "        # 2. Schema Management: Persistence is key for Unity Catalog.\n",
    "        # This allows the stream to restart and remember the previous schema.\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "\n",
    "        # 3. Precision Control: Force the IoT timestamp to high-precision TIMESTAMP \n",
    "        # to prevent it being misidentified as a STRING during inference.\n",
    "        .option(\"cloudFiles.schemaHints\", \"ingest_timestamp TIMESTAMP\")\n",
    "\n",
    "        # 4. Processing Strategy: 'Incremental-only' mode. \n",
    "        # Ignore existing backlogs in the bucket and process only data arriving from now.\n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"false\")  #set to true to process existing files and false to ignore existing files\n",
    "\n",
    "        # 5. Source Path: Map to the Unity Catalog External Volume.\n",
    "        .load(cloud_path)\n",
    "\n",
    "        # --- AUDIT & LINEAGE METADATA ---\n",
    "        # Capture the processing time and source file path for debugging and data lineage.\n",
    "        .withColumn(\"_ingest_timestamp\", current_timestamp())\n",
    "        .withColumn(\"_ingest_file_name\", col('_metadata.file_name'))\n",
    "        .withColumn(\"_ingest_file_path\", col('_metadata.file_path'))\n",
    "\n",
    "\n",
    "        # --- PARTITIONING & OPTIMIZATION ---\n",
    "        # Generate a DATE column from the timestamp. \n",
    "        # This will be used for partitioning the Bronze table to optimize storage and cost.\n",
    "        .withColumn(\"ingestion_date\", to_date(\"_ingest_timestamp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2142750",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\") # Explicitly define that we are only adding new rows\n",
    "        \n",
    "        # 1. Reliability: Essential for tracking progress in External Volumes\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "\n",
    "        # 2. Evolution: Ensures the Delta Table structure updates when sensors change\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "\n",
    "        # 3. Layout: Physcially organizes data on disk by date for faster cleanup/queries\n",
    "        .partitionBy(\"ingestion_date\")\n",
    "\n",
    "        # 4. Cost Control: Processes all pending files and then stops the cluster\n",
    "        .trigger(availableNow=True)\n",
    "\n",
    "        # 5. Destination: The Unity Catalog Three-Level Namespace (catalog.schema.table)\n",
    "        .toTable(bronze_table) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ingestion Job Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
