{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession  #ruung in db\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from local_time.local_utils import add_local_time\n",
    "from day_time.day_utils import add_daytime_flag\n",
    "\n",
    "#from src.local_time.local_utils import add_local_time -> build a wheel and import from there\n",
    "#from src.day_time.day_utils import add_daytime_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f82614de",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema_landing = dbutils.widgets.get(\"schema_landing\")\n",
    "schema_bronze = dbutils.widgets.get(\"schema_bronze\")\n",
    "schema_silver = dbutils.widgets.get(\"schema_silver\")\n",
    "volume = dbutils.widgets.get(\"volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a385654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Load Bronze Delta table (append-only raw ingestion layer)\n",
    "# 1. INCREMENTAL READ\n",
    "# ------------------------------------------------------------\n",
    "# WHY:\n",
    "\n",
    "# Silver ALWAYS reads incrementally from Bronze.\n",
    "# Bronze should contain raw nested JSON + metadata columns.\n",
    "# Here we assume your Bronze table is:\n",
    "#   iot_catalog.bronze.weather_raw\n",
    "# ------------------------------------------------------------\n",
    "bronze_df = spark.readStream.table(f\"{catalog}.{schema_bronze}.iot_bronze_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = bronze_df.select(\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 1. Identity & Location\n",
    "    # ----------------------------------------------------------------\n",
    "    F.sha2(F.concat_ws(\"_\", F.col(\"id\"), F.col(\"dt\")), 256).alias(\"event_uuid\"),\n",
    "    F.col(\"id\").cast(T.StringType()).alias(\"city_id\"),\n",
    "    F.col(\"name\").cast(T.StringType()).alias(\"name\"),\n",
    "    F.col(\"sys.country\").cast(T.StringType()).alias(\"country\"),\n",
    "    F.col(\"coord.lat\").cast(T.DecimalType(10, 2)).alias(\"latitude\"),\n",
    "    F.col(\"coord.lon\").cast(T.DecimalType(10, 2)).alias(\"longitude\"),\n",
    "    F.col(\"timezone\").cast(T.IntegerType()).alias(\"timezone\"),\n",
    "\n",
    "  # ----------------------------------------------------------------\n",
    "    # 2. Time Dimensions (The \"When\")\n",
    "    # ----------------------------------------------------------------\n",
    "    # Note: These are timestamps, converted from Unix\n",
    "    F.col(\"dt\").cast(T.TimestampType()).alias(\"datetime\"),\n",
    "    F.col(\"sys.sunrise\").cast(T.TimestampType()).alias(\"sunrise\"),\n",
    "    F.col(\"sys.sunset\").cast(T.TimestampType()).alias(\"sunset\"),\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3. Weather Metrics - Main\n",
    "    # ----------------------------------------------------------------\n",
    "    F.col(\"main.temp\").cast(T.DecimalType(10, 2)).alias(\"temperature\"),\n",
    "    F.col(\"main.feels_like\").cast(T.DecimalType(10, 2)).alias(\"feels_like\"),\n",
    "    F.col(\"main.temp_min\").cast(T.DecimalType(10, 2)).alias(\"temperature_min\"),\n",
    "    F.col(\"main.temp_max\").cast(T.DecimalType(10, 2)).alias(\"temperature_max\"),\n",
    "    F.col(\"main.humidity\").cast(T.IntegerType()).alias(\"humidity\"),\n",
    "    F.col(\"main.pressure\").cast(T.IntegerType()).alias(\"pressure\"),\n",
    "    F.col(\"main.sea_level\").cast(T.IntegerType()).alias(\"main_sea_level\"),\n",
    "    F.col(\"main.grnd_level\").cast(T.IntegerType()).alias(\"main_grnd_level\"),\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 4. Weather Metrics - Atmosphere \n",
    "    # note ->  weather is an array of structs, we take the first struct (index 0) which contains the main weather info\n",
    "    # ----------------------------------------------------------------\n",
    "    F.col(\"weather\")[0][\"main\"].cast(T.StringType()).alias(\"weather_main\"),\n",
    "    F.col(\"weather\")[0][\"description\"].cast(T.StringType()).alias(\"weather_description\"),\n",
    "    F.col(\"weather\")[0][\"icon\"].cast(T.StringType()).alias(\"weather_icon\"),\n",
    "    F.col(\"weather\")[0][\"id\"].cast(T.IntegerType()).alias(\"weather_id\"),\n",
    "    F.col(\"visibility\").cast(T.IntegerType()).alias(\"visibility\"),\n",
    "    F.col(\"clouds.all\").cast(T.IntegerType()).alias(\"clouds_all\"),\n",
    "    F.col(\"rain.1h\").cast(T.DecimalType(10, 2)).alias(\"rain_1h\"),\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 5. Weather Metrics - Wind\n",
    "    # ----------------------------------------------------------------\n",
    "    F.col(\"wind.speed\").cast(T.DecimalType(10, 2)).alias(\"windspeed\"),\n",
    "    F.col(\"wind.gust\").cast(T.DecimalType(10, 2)).alias(\"wind_gust\"),\n",
    "    F.col(\"wind.deg\").cast(T.IntegerType()).alias(\"wind_deg\"),\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 6. Technical Metadata\n",
    "    # ----------------------------------------------------------------\n",
    "    F.col(\"base\").cast(T.StringType()).alias(\"base\"),\n",
    "    F.col(\"cod\").cast(T.IntegerType()).alias(\"cod\"),\n",
    "    F.col(\"sys.id\").cast(T.IntegerType()).alias(\"sys_id\"),\n",
    "    F.col(\"sys.type\").cast(T.IntegerType()).alias(\"sys_type\"),\n",
    "    F.col(\"ingestion_date\").cast(T.DateType()).alias(\"ingestion_date\"),\n",
    "    \n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 7. Ingestion Metadata\n",
    "    # ----------------------------------------------------------------\n",
    "    \n",
    "    F.struct(\n",
    "        F.col(\"ingest_timestamp\").cast(T.TimestampType()).alias(\"timestamp\"),\n",
    "        F.col(\"_ingest_file_name\").cast(T.StringType()).alias(\"file_name\"),\n",
    "        F.col(\"_ingest_file_path\").cast(T.StringType()).alias(\"file_path\"),\n",
    "        F.col(\"_rescued_data\").alias(\"rescued_data\")\n",
    "        ).alias(\"metadata\")   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a function to add local time based on the timezone offset provided in the API response. This enriches our data with a local timestamp for each weather observation, \n",
    "# which can be useful for time-based analyses and visualizations in the local context of the weather event.  \n",
    "df_new = add_local_time(\n",
    "    df=df_new, \n",
    "    datetime_col=\"datetime\", \n",
    "    timezone_col=\"timezone\",\n",
    "    output_col=\"local_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a function to add a daytime flag based on the sunrise and sunset times provided in the API response. \n",
    "# This enriches our data with a boolean indicator of whether each weather observation occurred during daytime or nighttime, \n",
    "# which can be useful for analyses that differentiate between day and night conditions.\n",
    "df_normalized = add_daytime_flag(\n",
    "    df=df_new, \n",
    "    datetime_col=\"datetime\", \n",
    "    sunrise_col=\"sunrise\",\n",
    "    sunset_col=\"sunset\",\n",
    "    output_col=\"is_daytime\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3811f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# 3. DATA QUALITY (DQ) ASSESSMENT\n",
    "# =====================================================================================\n",
    "# Instead of hard filtering immediately, we flag rows. \n",
    "# This helps with observability (knowing HOW MANY rows are bad).\n",
    "#ensure the latitude and longitude are within valid ranges, ensure timestamps are not null, and ensure temperature is not null (indicating a working sensor).\n",
    "rule_geo = (F.col(\"latitude\").between(-90, 90)) & (F.col(\"longitude\").between(-180, 180))\n",
    "rule_time = F.col(\"datetime\").isNotNull()\n",
    "rule_temp = F.col(\"temperature\").isNotNull()\n",
    "\n",
    "dq_df = (\n",
    "    df_normalized\n",
    "    # --- A. Apply Individual Flags ---\n",
    "    .withColumn(\"is_valid_geo\", rule_geo)\n",
    "    .withColumn(\"is_valid_timestamp\", rule_time)\n",
    "    .withColumn(\"is_valid_sensor\", rule_temp)\n",
    "    \n",
    "    # --- B. The Master Flag ---\n",
    "    .withColumn(\"dq_status\",\n",
    "        F.when(rule_geo & rule_time & rule_temp, F.lit(\"PASS\"))\n",
    "         .otherwise(F.lit(\"FAIL\"))\n",
    "    )\n",
    "    \n",
    "    # --- C. The \"Why did it fail?\" Column (Expert Addition) ---\n",
    "    # This concatenates error codes so you can debug later.\n",
    "    # Example Output: \"INVALID_GEO; NULL_TEMP\"\n",
    "    .withColumn(\"failure_reason\",\n",
    "        F.concat_ws(\"; \",\n",
    "            F.when(~rule_geo, F.lit(\"INVALID_GEO\")),\n",
    "            F.when(~rule_time, F.lit(\"NULL_TIME\")),\n",
    "            F.when(~rule_temp, F.lit(\"NULL_TEMP\"))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "903bf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 4. FILTERING\n",
    "# =====================================================================================\n",
    "\n",
    "# 4a. Filter for Silver (We only want Clean data here)\n",
    "# Note: In a larger system, the \"FAIL\" rows would be written to a Quarantine table.\n",
    "clean_df = dq_df.filter(F.col(\"dq_status\") == \"PASS\").drop(\"dq_status\", \"is_valid_geo\", \"is_valid_timestamp\", \"is_valid_sensor\",\"failure_reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a76111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Select everything EXCEPT metadata, then add metadata at the end\"\n",
    "df_final = clean_df.select(\n",
    "    *[c for c in clean_df.columns if c != 'metadata'], \n",
    "    'metadata'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b5d479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UC Volume used for checkpoints (stream state) #volume/catalog/schemas\n",
    "checkpoint_path = f\"/Volumes/{catalog}/{schema_landing}/{volume}/checkpoints/iot_silver/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e510989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.streaming.query.StreamingQuery at 0x1e1f52bed80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 5. WRITE TO SILVER\n",
    "# =====================================================================================\n",
    "(\n",
    "df_final.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        # Trigger AvailableNow processes all pending data then stops (Cost efficient)\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(f\"{catalog}.{schema_silver}.weather_clean\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
