{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f31624e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\sdk\\config.py:676\u001b[39m, in \u001b[36mConfig.init_auth\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[38;5;28mself\u001b[39m._header_factory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_credentials_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28mself\u001b[39m.auth_type = \u001b[38;5;28mself\u001b[39m._credentials_strategy.auth_type()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\sdk\\credentials_provider.py:1170\u001b[39m, in \u001b[36mDefaultCredentials.__call__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m   1169\u001b[39m auth_flow_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1171\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot configure default credentials, please check \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauth_flow_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to configure credentials for your preferred authentication method.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1172\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\sdk\\config.py:273\u001b[39m, in \u001b[36mConfig.__init__\u001b[39m\u001b[34m(self, credentials_provider, credentials_strategy, product, product_version, clock, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m._validate()\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m._init_product(product, product_version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\sdk\\config.py:681\u001b[39m, in \u001b[36mConfig.init_auth\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._credentials_strategy.auth_type()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m auth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatabricks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatabricksSession  \u001b[38;5;66;03m#ruung in db\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m spark = \u001b[43mDatabricksSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\connect\\session.py:416\u001b[39m, in \u001b[36mDatabricksSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetOrCreate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> SparkSession:\n\u001b[32m    397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    Get an existing :class:SparkSession for the provided configuration or, if there is no\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[33;03m     existing one, create a new one.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m \u001b[33;03m        queries to this spark session are executed remotely.\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\connect\\session.py:539\u001b[39m, in \u001b[36mDatabricksSession.Builder._create\u001b[39m\u001b[34m(self, _Builder__skip_cache)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# use the default values that may be supplied from the SDK Config\u001b[39;00m\n\u001b[32m    538\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mFalling back to default configuration from the SDK.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m config = \u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._from_sdkconfig(\n\u001b[32m    541\u001b[39m     config, \u001b[38;5;28mself\u001b[39m._gen_user_agent(), \u001b[38;5;28mself\u001b[39m._headers,\n\u001b[32m    542\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_session_enabled, \u001b[38;5;28mself\u001b[39m._env,\n\u001b[32m    543\u001b[39m     \u001b[38;5;28mself\u001b[39m._usage_policy_id, __skip_cache=__skip_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TBR44\\Desktop\\databricks\\weather\\weather_iot_bundle\\venv_db\\Lib\\site-packages\\databricks\\sdk\\config.py:277\u001b[39m, in \u001b[36mConfig.__init__\u001b[39m\u001b[34m(self, credentials_provider, credentials_strategy, product, product_version, clock, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    276\u001b[39m     message = \u001b[38;5;28mself\u001b[39m.wrap_debug_info(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method."
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession  #ruung in db\n",
    "spark = DatabricksSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507239fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b16e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE BRONZE SCHEMA\n",
    "spark.sql(\"\"\"CREATE SCHEMA IF NOT EXISTS iot_catalog.02_silver\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a385654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Load Bronze Delta table (append-only raw ingestion layer)\n",
    "# ------------------------------------------------------------\n",
    "# WHY:\n",
    "# Silver ALWAYS reads incrementally from Bronze.\n",
    "# Bronze should contain raw nested JSON + metadata columns.\n",
    "# Here we assume your Bronze table is:\n",
    "#   iot_catalog.bronze.weather_raw\n",
    "# ------------------------------------------------------------\n",
    "bronze_df = spark.readStream.table(\"iot_catalog.01_bronze.iot_bronze_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. Select & flatten nested structures\n",
    "# ------------------------------------------------------------\n",
    "# WHY:\n",
    "# Silver removes nested dictionaries (coord, main, wind, sys, weather[])\n",
    "# because BI tools & SQL users cannot work with nested structures.\n",
    "# This is a mandatory Silver requirement.\n",
    "# ------------------------------------------------------------\n",
    "flattened_df = bronze_df.select(\n",
    "    # Business attributes (extracted from raw JSON)\n",
    "\n",
    "    col(\"id\").alias(\"city_id\"),                 # normalize naming\n",
    "    col(\"name\").alias(\"city_name\"),\n",
    "    col(\"base\"),                                # source type (stations)\n",
    "    col(\"timezone\").alias(\"timezone_offset\"),   # seconds offset\n",
    "    \n",
    "    # Geographic flattening\n",
    "    col(\"coord.lat\").alias(\"latitude\"),\n",
    "    col(\"coord.lon\").alias(\"longitude\"),\n",
    "\n",
    "    # Weather main metrics\n",
    "    col(\"main.temp\").alias(\"temperature_celsius\"),\n",
    "    col(\"main.pressure\").alias(\"pressure_hpa\"),\n",
    "    col(\"main.humidity\").alias(\"humidity_pct\"),\n",
    "    col(\"main.temp_min\").alias(\"temp_min_celsius\"),\n",
    "    col(\"main.temp_max\").alias(\"temp_max_celsius\"),\n",
    "\n",
    "    # Weather description (first element of array)\n",
    "    col(\"weather\")[0][\"main\"].alias(\"weather_main\"),\n",
    "    col(\"weather\")[0][\"description\"].alias(\"weather_description\"),\n",
    "    col(\"weather\")[0][\"icon\"].alias(\"weather_icon\"),\n",
    "\n",
    "    # Wind flattening\n",
    "    col(\"wind.deg\").alias(\"wind_direction_deg\"),\n",
    "    col(\"wind.speed\").alias(\"wind_speed_ms\"),\n",
    "\n",
    "    # Timestamp from payload\n",
    "    (col(\"dt\")).cast(\"timestamp\").alias(\"source_event_time\"),\n",
    "\n",
    "    # Rain flattening (if present)\n",
    "    col(\"rain.1h\").alias(\"rain_1h_mm\"),\n",
    "    col(\"rain.3h\").alias(\"rain_3h_mm\"),\n",
    "\n",
    "    # Bronze metadata REQUIRED in Silver\n",
    "    col(\"_ingest_timestamp\"),\n",
    "    col(\"_ingest_file_name\"),\n",
    "    col(\"ingestion_date\"),\n",
    "    \n",
    "    # Add Silver processing timestamp\n",
    "    current_timestamp().alias(\"_processed_timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. Data Quality Enforcement (Silver requirement #2)\n",
    "# ------------------------------------------------------------\n",
    "# Examples applied here:\n",
    "#  - Ensure latitude/longitude are valid\n",
    "#  - Remove records missing critical weather values\n",
    "#  - Drop negative humidity or pressure\n",
    "#  - Ensure temperature is within realistic bounds\n",
    "#\n",
    "# In real production youâ€™d add many more expectations.\n",
    "# ------------------------------------------------------------\n",
    "clean_df = flattened_df.filter(\n",
    "    (col(\"latitude\").isNotNull()) &\n",
    "    (col(\"longitude\").isNotNull()) &\n",
    "    (col(\"temperature_celsius\").between(-100, 100)) &\n",
    "    (col(\"humidity_pct\").between(0, 100)) &\n",
    "    (col(\"pressure_hpa\") > 300) &\n",
    "    (col(\"pressure_hpa\") < 1200)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. Deduplication (Silver requirement #4)\n",
    "# ------------------------------------------------------------\n",
    "# Dedupe key for weather data:\n",
    "# city_id + source_event_time uniquely identifies a weather reading.\n",
    "# We use windowing + row_number.\n",
    "# ------------------------------------------------------------\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "dedupe_window = Window.partitionBy(\n",
    "    \"city_id\", \"source_event_time\"\n",
    ").orderBy(\n",
    "    col(\"_ingest_timestamp\").desc()\n",
    ")\n",
    "\n",
    "deduped_df = clean_df.withColumn(\n",
    "    \"row_num\", row_number().over(dedupe_window)\n",
    ").filter(\"row_num = 1\").drop(\"row_num\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5. Survivorship & Standardization (Silver #6)\n",
    "# ------------------------------------------------------------\n",
    "# Apply:\n",
    "#  - timezone standardization (convert to UTC)\n",
    "#  - normalize strings (trim, lower-case)\n",
    "#  - ensure consistent units\n",
    "# ------------------------------------------------------------\n",
    "standard_df = deduped_df.select(\n",
    "    \"*\",\n",
    "    # Convert weather to lowercase for consistency\n",
    "    lower(col(\"weather_main\")).alias(\"weather_main_std\"),\n",
    "    lower(col(\"weather_description\")).alias(\"weather_description_std\"),\n",
    "\n",
    "    # Standardizing city names\n",
    "    initcap(col(\"city_name\")).alias(\"city_name_std\"),\n",
    "\n",
    "    # Convert source timestamp + timezone to UTC (canonical time)\n",
    "    (col(\"source_event_time\") - expr(\"INTERVAL timezone_offset SECOND\")).alias(\"event_time_utc\")\n",
    ").drop(\"weather_main\", \"weather_description\", \"city_name\")  # remove unstandardized values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff25608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. Incremental Silver Write (Silver #7)\n",
    "# ------------------------------------------------------------\n",
    "# Silver MUST be incremental, never full-refresh.\n",
    "# Always write to a managed Delta table with ACID guarantees.\n",
    "# ------------------------------------------------------------\n",
    "(\n",
    "    standard_df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", \"/Volumes/iot_catalog/silver/checkpoints/weather/\")\n",
    "        .trigger(availableNow=True)                      # runs once per new Bronze batch\n",
    "        .table(\"iot_catalog.silver.weather_clean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10efce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7. Optional Delta Optimization (Silver #8)\n",
    "# ------------------------------------------------------------\n",
    "# Run manually or via a scheduled job:\n",
    "# spark.sql(\"OPTIMIZE iot_catalog.silver.weather_clean ZORDER BY (event_time_utc)\")\n",
    "# spark.sql(\"VACUUM iot_catalog.silver.weather_clean RETAIN 168 HOURS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7c45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
